# awesome-video-generation <!-- 2D person independent / dependent-->

## Company
**InfinityStar: Unified Spacetime AutoRegressive Modeling for Visual Generation**<br>
*ByteDance.*<br>
ByteDance 2025. [[PDF](https://arxiv.org/pdf/2511.04675v1)] [[Code](https://github.com/FoundationVision/InfinityStar)] [[Project](https://github.com/FoundationVision/InfinityStar)]

**Wan: Open and Advanced Large-Scale Video Generative Models**<br>
*Wan Team, Alibaba Group.*<br>
Alibaba 2025. [[PDF](https://arxiv.org/pdf/2503.20314)] [[Code](https://github.com/Wan-Video/Wan2.1)] [[Project](https://wan.video/)]

**Movie Gen: A Cast of Media Foundation Models**<br>
*The Movie Gen team @ Meta.*<br>
2025. [[PDF](https://arxiv.org/pdf/2410.13720)] [[Project](https://go.fb.me/MovieGenResearchVideos)]

**Emu Video: Factorizing Text-to-Video Generation by Explicit Image Conditioning**<br>
*Rohit Girdhar, Mannat Singh, Andrew Brown, Quentin Duval, Samaneh Azadi, Sai Saketh Rambhatla, Akbar Shah, Xi Yin, Devi Parikh, Ishan Misra.*<br>
ECCV 2024. [[PDF](https://arxiv.org/pdf/2311.10709)] [[Project](https://emu-video.metademolab.com/)]

**Emu Video: Factorizing Text-to-Video Generation by Explicit Image Conditioning**<br>
*Rohit Girdhar, Mannat Singh, Andrew Brown, Quentin Duval, Samaneh Azadi, Sai Saketh Rambhatla, Akbar Shah, Xi Yin, Devi Parikh, Ishan Misra.*<br>
ECCV 2024. [[PDF](https://arxiv.org/pdf/2311.10709)] [[Project](https://emu-video.metademolab.com/)]

**Sora**<br>
*Yixin Liu, Kai Zhang, Yuan Li, Zhiling Yan, Chujie Gao, Ruoxi Chen, Zhengqing Yuan, Yue Huang, Hanchi Sun, Jianfeng Gao, Lifang He, Lichao Sun.*<br>
OpenAI 2024.


**Imagen Video: High Definition Video Generation With Diffusion Models**<br>
*Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P. Kingma, Ben Poole, Mohammad Norouzi, David J. Fleet, Tim Salimans.*<br>
Google Research 2023. [[PDF](https://arxiv.org/abs/2210.02303)] [[Code](https://github.com/lucidrains/imagen-pytorch)] [[Project](https://imagen.research.google/video/)]

**Phenaki: Variable Length Video Generation from Open Domain Textual Descriptions**<br>
*Ruben Villegas, Mohammad Babaeizadeh, Pieter-Jan Kindermans, Hernan Moraldo, Han Zhang, Mohammad Taghi Saffar, Santiago Castro, Julius Kunze, Dumitru Erhan.*<br>
ICLR 2023. [[PDF](https://openreview.net/pdf?id=vOEXS39nOF)] [[Code](https://github.com/lucidrains/phenaki-pytorch)] [[Project](https://phenaki.video/#)]

## Unified Generation
**Wan-Animate: Unified Character Animation and Replacement with Holistic Replication**<br>
*Gang Cheng, Xin Gao, Li Hu, Siqi Hu, Mingyang Huang, Chaonan Ji, Ju Li, Dechao Meng, Jinwei Qi, Penchong Qiao, Zhen Shen, Yafei Song, Ke Sun, Linrui Tian, Feng Wang, Guangyuan Wang, Qi Wang, Zhongjian Wang, Jiayu Xiao, Sheng Xu, Bang Zhang, Peng Zhang, Xindi Zhang, Zhe Zhang, Jingren Zhou, Lian Zhuo.*<br>
Alibaba 2025. [[PDF](https://arxiv.org/pdf/2509.14055)] [[Code](https://github.com/Wan-Video/Wan2.2)] [[Project](https://humanaigc.github.io/wan-animate/)]



## Audio-to-video generation / 
### Facial animation
**Avatar Forcing: Real-Time Interactive Head Avatar Generation for Natural Conversation**<br>
*Taekyung Ki, Sangwon Jang, Jaehyeong Jo, Jaehong Yoon, Sung Ju Hwang.*<br>
2025. [[PDF](https://arxiv.org/pdf/2601.00664)] [[Project](https://taekyungki.github.io/AvatarForcing/)] [[Code](https://github.com/TaekyungKi/AvatarForcing)]

**Lookahead Anchoring: Preserving Character Identity in Audio-Driven Human Animation**<br>
*Junyoung Seo, Rodrigo Mira, Alexandros Haliassos, Stella Bounareli, Honglie Chen, Linh Tran, Seungryong Kim, Zoe Landgraf, Jie Shen.*<br>
2025. [[PDF](https://www.arxiv.org/pdf/2510.23581)] [[Project](https://lookahead-anchoring.github.io/)]


**OmniHuman-1.5: Instilling an Active Mind in Avatars via Cognitive Simulation**<br>
*ByteDance.*<br>
2025. [[PDF](https://arxiv.org/pdf/2508.19209v1)] [[Project](https://omnihuman-lab.github.io/v1_5)]


**WAN-S2V: AUDIO-DRIVEN CINEMATIC VIDEO GENERATION**<br>
*Alibaba.*<br>
2025. [[PDF](https://arxiv.org/pdf/2508.18621v1)] [[Project](https://omnihuman-lab.github.io/v1_5)]


**Kling-Avatar: Grounding Multimodal Instructions for Cascaded Long-Duration Avatar Animation Synthesis**<br>
*Kuaishou.*<br>
2025. [[PDF](https://arxiv.org/pdf/2509.09595v1)]


**MoDiTalker: Motion-Disentangled Diffusion Model for High-Fidelity Talking Head Generation**<br>
*Seyeon Kim, Siyoon Jin, Jihye Park, Kihong Kim, Jiyoung Kim, Jisu Nam, Seungryong Kim.*<br>
AAAI 2025. [[PDF](https://arxiv.org/pdf/2403.19144)] [[Projects](https://cvlab-kaist.github.io/MoDiTalker/)] [[Code](https://github.com/KU-CVLAB/MoDiTalker)]

**EchoMimicV3: Towards Striking, Simplified, and Semi-Body Human Animation**<br>
*Rang Meng, Yan Wang, Weipeng Wu, Ruobing Zheng, Yuming Li, Chenguang Ma.*<br>
arXiv 2025. [[PDF](https://arxiv.org/pdf/2507.03905)] [[Projects](https://antgroup.github.io/ai/echomimic_v3/)] [[Code](https://github.com/antgroup/echomimic_v3)]

**EchoMimicV2: Towards Striking, Simplified, and Semi-Body Human Animation**<br>
*Rang Meng, Xingyu Zhang, Yuming Li, Chenguang Ma.*<br>
AAAI 2025. [[PDF](https://arxiv.org/pdf/2411.10061)] [[Projects](https://antgroup.github.io/ai/echomimic_v2/)] [[Code](https://github.com/antgroup/echomimic_v2)]

**Hallo3: Highly Dynamic and Realistic Portrait Image Animation with Video Diffusion Transformer**<br>
*Jiahao Cui, Hui Li, Yao Yao, Hao Zhu, Hanlin Shang, Kaihui Cheng, Hang Zhou, Siyu Zhu, Jingdong Wang.*<br>
CVPR 2025. [[PDF](https://arxiv.org/pdf/2412.00733)] [[Projects](https://fudan-generative-vision.github.io/hallo3/#/)] [[Code](https://github.com/fudan-generative-vision/hallo3)]

**Hallo2: Long-Duration and High-Resolution Audio-Driven Portrait Image Animation**<br>
*Jiahao Cui, Hui Li, Yao Yao, Hao Zhu, Hanlin Shang, Kaihui Cheng, Hang Zhou, Siyu Zhu, Jingdong Wang.*<br>
ICLR 2025. [[PDF](https://arxiv.org/pdf/2410.07718)] [[Projects](https://fudan-generative-vision.github.io/hallo2/#/)] [[Code](https://github.com/fudan-generative-vision/hallo2)]

**EchoMimic: Lifelike Audio-Driven Portrait Animations through Editable Landmark Conditioning**<br>
*Zhiyuan Chen, Jiajiong Cao, Zhiquan Chen, Yuming Li, Chenguang Ma.*<br>
AAAI 2025. [[PDF](https://arxiv.org/pdf/2407.08136)] [[Projects](https://badtobest.github.io/echomimic.html)] [[Code](https://github.com/antgroup/echomimic)]

**EMO2: End-Effector Guided Audio-Driven Avatar Video Generation**<br>
*Linrui Tian, Siqi Hu, Qi Wang, Bang Zhang, Liefeng Bo.*<br>
arXiv 2025. [[PDF](https://arxiv.org/pdf/2402.17485)] [[Projects](https://humanaigc.github.io/emote-portrait-alive-2/)]


**VASA-1: Lifelike Audio-Driven Talking Faces Generated in Real Time**<br>
*Sicheng Xu, Guojun Chen, Yu-Xiao Guo, Jiaolong Yang, Chong Li, Zhenyu Zang, Yizhong Zhang, Xin Tong, Baining Guo.*<br>
arXiv 2024. [[PDF](https://arxiv.org/pdf/2404.10667)] [[Projects](https://www.microsoft.com/en-us/research/project/vasa-1/)] 

**Hallo: Hierarchical Audio-Driven Visual Synthesis for Portrait Image Animation**<br>
*Wenxuan Zhang, Xiaodong Cun, Xuan Wang, Yong Zhang, Xi Shen, Yu Guo, Ying Shan, Fei Wang.*<br>
arXiv 2024. [[PDF](https://arxiv.org/pdf/2406.08801)] [[Projects](https://fudan-generative-vision.github.io/hallo/#/)] [[Code](https://github.com/fudan-generative-vision/hallo)]

**Aniportrait: Audio-driven synthesis of photorealistic portrait animation**<br>
*Huawei Wei, Zejun Yang, Zhisheng Wang.*<br>
arXiv 2024. [[PDF](https://arxiv.org/pdf/2403.17694)] [[Code](https://github.com/Zejun-Yang/AniPortrait)]

**V-Express: Conditional Dropout for Progressive Training of Portrait Video Generation**<br>
*Cong Wang, Kuan Tian, Jun Zhang, Yonghang Guan, Feng Luo, Fei Shen, Zhiwei Jiang, Qing Gu, Xiao Han, Wei Yang.*<br>
arXiv 2024. [[PDF](https://arxiv.org/pdf/2406.02511)] [[Projects](https://tenvence.github.io/p/v-express/)] [[Code](https://github.com/tencent-ailab/V-Express)]


**EMO: Emote Portrait Alive - Generating Expressive Portrait Videos with Audio2Video Diffusion Model under Weak Conditions**<br>
*Linrui Tian, Qi Wang, Bang Zhang, Liefeng Bo.*<br>
ECCV 2024. [[PDF](https://arxiv.org/pdf/2402.17485)] [[Projects](https://humanaigc.github.io/emote-portrait-alive/)] [[Code](https://github.com/HumanAIGC/EMO)]

**SadTalker: Learning Realistic 3D Motion Coefficients for Stylized Audio-Driven Single Image Talking Face Animation**<br>
*Ziqiao Peng, Wentao Hu, Yue Shi, Xiangyu Zhu, Xiaomei Zhang, Hao Zhao, Jun He, Hongyan Liu, Zhaoxin Fan.*<br>
CVPR 2023. [[PDF](https://arxiv.org/pdf/2211.12194)] [[Projects](https://sadtalker.github.io/)] [[Code](https://github.com/Winfredy/SadTalker)]

**High-Fidelity and Freely Controllable Talking Head Video Generation**<br>
*Yue Gao, Yuan Zhou, Jinglu Wang, Xiao Li, Xiang Ming, Yan Lu.*<br>
CVPR 2023. [[PDF](https://yuegao.me/PECHead/paper.pdf)] [[Projects](https://yuegao.me/PECHead/)] 

**DAE-Talker: High Fidelity Speech-Driven Talking Face Generation with Diffusion Autoencoder**<br>
*Wenxuan Zhang, Xiaodong Cun, Xuan Wang, Yong Zhang, Xi Shen, Yu Guo, Ying Shan, Fei Wang.*<br>
ACM 2023. [[PDF](https://arxiv.org/pdf/2303.17550)] [[Projects](https://daetalker.github.io/)] 

**DiffTalk: Crafting Diffusion Models for Generalized Audio-Driven Portraits Animation**<br>
*Shuai Shen, Wenliang Zhao, Zibin Meng, Wanhua Li, Zheng Zhu, Jie Zhou, Jiwen Lu.*<br>
CVPR 2023. [[PDF](https://arxiv.org/pdf/2301.03786)] [[Projects](https://sstzal.github.io/DiffTalk/)] [[Code](https://github.com/sstzal/DiffTalk)]

**Diffused Heads: Diffusion Models Beat GANs on Talking-Face Generation**<br>
*Wenxuan Zhang, Xiaodong Cun, Xuan Wang, Yong Zhang, Xi Shen, Yu Guo, Ying Shan, Fei Wang.*<br>
WACV 2024. [[PDF](https://mstypulkowski.github.io/diffusedheads/diffused_heads.pdf)] [[Projects](https://mstypulkowski.github.io/diffusedheads/)] [[Code](https://github.com/MStypulkowski/diffused-heads)]

**StyleTalk: One-shot Talking Head Generation with Controllable Speaking Styles**<br>
*Yifeng Ma, Suzhen Wang, Zhipeng Hu, Changjie Fan, Tangjie Lv, Yu Ding, Zhidong Deng, Xin Yu.*<br>
AAAI 2023. [[PDF](https://arxiv.org/pdf/2301.01081)] [[Projects](https://github.com/FuxiVirtualHuman/styletalk)] [[Code](https://github.com/FuxiVirtualHuman/styletalk)]

### Lipsync
**StyleSync: High-Fidelity Generalized and Personalized Lip Sync in Style-based Generator**<br>
*Taekyung Ki, Dongchan Min.*<br>
ICCV 2023. [[PDF](https://arxiv.org/pdf/2305.05445)] [[Projects](https://hangz-nju-cuhk.github.io/projects/StyleSync)] [[Code](https://github.com/guanjz20/StyleSync)]



**StyleLipSync: Style-based Personalized Lip-sync Video Generation**<br>
*Taekyung Ki, Dongchan Min.*<br>
ICCV 2023. [[PDF](https://arxiv.org/pdf/2305.00521)] [[Projects](https://stylelipsync.github.io/)] [[Code](https://github.com/TaekyungKi/StyleLipSync)]


## Text-to-video generation
**Make-A-Video: Text-to-Video Generation without Text-Video Data**<br>
*Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, Devi Parikh, Sonal Gupta, Yaniv Taigman.*<br>
Meta 2023. [[PDF](https://openreview.net/pdf?id=nJfylDvgzlq)] [[Project](https://make-a-video.github.io/)]

**Video Diffusion Models**<br>
*Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, David J. Fleet*<br>
Google 2022. [[PDF](https://arxiv.org/pdf/2204.03458)] [[Project](https://video-diffusion.github.io)]


## Pose-to-video generation
**Animate Anyone: Consistent and Controllable Image-to-Video Synthesis for Character Animation**<br>
*Li Hu, Xin Gao, Peng Zhang, Ke Sun, Bang Zhang, Liefeng Bo.*<br>
Alibaba 2025. [[PDF](https://arxiv.org/pdf/2311.17117)] [[Code](https://github.com/HumanAIGC/AnimateAnyone)] [[Project](https://humanaigc.github.io/animate-anyone/)]

## Controllable Video Generation
**ATI: Any Trajectory Instruction for Controllable Video Generation**<br>
*Angtian Wang, Haibin Huang, Jacob Zhiyuan Fang, Yiding Yang, Chongyang Ma.*<br>
ICLR 2023. [[PDF](https://arxiv.org/pdf/2505.22944)] [[Code](https://github.com/bytedance/ATI)] [[Project](https://anytraj.github.io/)]



## Evaluation

**POSE: Phased One-Step Adversarial Equilibrium for Video Diffusion Models**<br>
*Jiaxiang Cheng, Bing Ma, Xuhua Ren, Hongyi Jin, Kai Yu, Peng Zhang, Wenyue Li, Yuan Zhou, Tianxiang Zheng, Qinglin Lu.*<br>
Tencent Hunyuan 2025. [[PDF](https://arxiv.org/pdf/2508.21019v1)] [[Project](https://pose-paper.github.io/)]




## Video Understanding
**Character-Centric Understanding of Animated Movies**<br>
*VGG.*<br>
VGG 2025. [[PDF](https://arxiv.org/pdf/2509.12204v1)]


## World Model
**OmniWorld: A Multi-Domain and Multi-Modal Dataset for 4D World Modeling**<br>
*Shanghai AI Lab, ZJU.*<br>
2025. [[PDF](https://arxiv.org/pdf/2509.12201v1)] [[Project](https://yangzhou24.github.io/OmniWorld/)] [[Code](https://github.com/yangzhou24/OmniWorld)]
